{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2017)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import glob\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "from skimage import transform, util\n",
    "from skimage import filters, color\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import __version__ as keras_version\n",
    "keras_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#size of image\n",
    "img_size = 128\n",
    "#num of collor changls \n",
    "num_channels = 3\n",
    "#conv filter row and col\n",
    "f_row = f_col = 3\n",
    "#number of filters for convolution layer1\n",
    "num_filter1 = 16\n",
    "#number of filters for convolution layer2\n",
    "num_filter2 = 36\n",
    "\n",
    "#folders = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and resizeing images using OpenCv - cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_img_cv2(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    #print(img.shape)\n",
    "    #img = util.img_as_float(img)\n",
    "    #eimg =  filters.sobel(color.rgb2gray(img)) #signify importance of fixes\n",
    "    #gray = color.rgb2gray(img)\n",
    "    #out = transform.seam_carve(gray, eimg, 'vertical',  500 )\n",
    "   \n",
    "    resized = cv2.resize(img, (img_size, img_size), cv2.INTER_LINEAR)\n",
    "    return resized\n",
    "    #return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path = os.path.join('input','train_img', '*.png')\n",
    "print(path)\n",
    "files = glob.glob(path)\n",
    "#print(files)\n",
    "fl = files[1]\n",
    "img = get_img_cv2(fl)\n",
    "print(img.shape)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Loading training data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "    train_images = []    \n",
    "    train_df = pd.read_csv('input/train.csv')\n",
    "    train_ids = train_df.image_id.values\n",
    "    train_labels = train_df.label.values    \n",
    "   \n",
    "    for img in tqdm(train_ids):\n",
    "        train_images.append(get_img_cv2('input/train_img/{}.png'.format(img)))            \n",
    "    \n",
    "    return train_images, train_labels, train_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_images, train_labels, train_ids = load_train_data()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_train = {k:v for v,k in enumerate(set(train_labels))}\n",
    "#y_train = [Y_train[k] for k in train_labels]  \n",
    "#len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(train_images[0])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_test_data():\n",
    "    \n",
    "    test_images = []\n",
    "    test_df = pd.read_csv('input/test.csv')\n",
    "    test_ids = test_df.image_id.values\n",
    "    \n",
    "    for img in tqdm(test_ids):\n",
    "        test_images.append(get_img_cv2('input/test_img/{}.png'.format(img)))\n",
    "    \n",
    "    return test_images, test_ids     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_images, test_id = load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(test_images[0])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read and normalize train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_normalize_train_data():\n",
    "    train_images, train_labels, train_ids = load_train_data()    \n",
    "    \n",
    "    print(\"Conver to numpy array\")\n",
    "    train_images = np.array(train_images, dtype = np.uint8)\n",
    "    \n",
    "    \n",
    "    print(\"Reshape train_images\")\n",
    "    train_images = train_images.transpose((0, 3, 1, 2))\n",
    "    \n",
    "    print(\"Normalize and Convert to float\")\n",
    "    train_images = train_images.astype('float32')\n",
    "    train_images = train_images / 255\n",
    "    \n",
    "    Y_train = {k:v for v,k in enumerate(set(train_labels))}\n",
    "    y_train = [Y_train[k] for k in train_labels]\n",
    "    train_labels = np_utils.to_categorical(y_train, len(Y_train))\n",
    "    train_labels = np.array(train_labels, dtype = np.uint8)\n",
    "    \n",
    "    print(\"Train shape :\", train_images.shape)\n",
    "    print(train_images.shape[0], \"train samples\")\n",
    "    return train_images, train_labels, Y_train, train_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read and normalize test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_and_normalize_test_data():\n",
    "    test_images, test_id = load_test_data()\n",
    "    \n",
    "    print(\"Convert to numpy array\")\n",
    "    test_images = np.array(test_images, dtype = np.uint8)\n",
    "    \n",
    "    print(\"Reshape test images\")\n",
    "    test_images = test_images.transpose((0,3,1,2))\n",
    "    #test_images = test_images.transpose((0,1,2)) #gray image\n",
    "    \n",
    "    print(\"Convert to float and normalize\")\n",
    "    test_images = test_images.astype('float32')\n",
    "    test_images = test_images / 255\n",
    "    \n",
    "    print(\"Test Shape: \", test_images.shape)\n",
    "    print(test_images.shape[0], \"test samples\")\n",
    "    return test_images, test_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D(padding=(1,1), input_shape = (num_channels, img_size, img_size), dim_ordering='th'))\n",
    "    model.add(Convolution2D(nb_filter=num_filter1, nb_row=f_row, nb_col=f_col, activation='relu', dim_ordering='th'))\n",
    "    model.add(ZeroPadding2D(padding=(1,1), dim_ordering='th'))\n",
    "    model.add(Convolution2D(nb_filter=num_filter1, nb_row=f_row, nb_col=f_col, activation='relu', dim_ordering='th'))          \n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), dim_ordering='th'))  \n",
    "    model.add(Dropout(0.25))          \n",
    "              \n",
    "    model.add(ZeroPadding2D(padding=(1,1), dim_ordering='th')) \n",
    "    model.add(Convolution2D(nb_filter=num_filter2, nb_row=f_row, nb_col=f_col, activation='relu', dim_ordering='th'))  \n",
    "    model.add(ZeroPadding2D(padding=(1,1), dim_ordering='th'))  \n",
    "    model.add(Convolution2D(nb_filter=num_filter2, nb_row=f_row, nb_col=f_col, activation='relu', dim_ordering='th'))  \n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), dim_ordering='th'))  \n",
    "    model.add(Dropout(0.25))  \n",
    "              \n",
    "    model.add(Flatten())   \n",
    "    model.add(Dense(output_dim=128, activation='relu'))  \n",
    "    model.add(Dropout(0.5))  \n",
    "    model.add(Dense(output_dim=128, activation='relu')) \n",
    "    model.add(Dropout(0.5))  \n",
    "    model.add(Dense(output_dim=25, activation='softmax')) \n",
    "              \n",
    "    sgd = SGD(lr=0.1, decay= 1e-6, momentum=0.9, nesterov=True)   \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics =['accuracy'])  \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Running Kfold cross validation on created model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfold_on_created_model(nfolds = 10):\n",
    "    #model fit input dimentions\n",
    "    batch_size = 32\n",
    "    nb_epoch = 10\n",
    "    random_state = 51\n",
    "    \n",
    "    train_images, train_labels, Y_train, train_ids = read_and_normalize_train_data()\n",
    "    yfull_train = dict()\n",
    "    kfold = KFold(len(train_images), n_folds=nfolds, shuffle=True, random_state=random_state)\n",
    "    num_folds = 0\n",
    "    sum_score = 0\n",
    "    models =[]\n",
    "    for train_idx, valid_idx in kfold:\n",
    "        model = create_model()\n",
    "        x_train = train_images[train_idx]\n",
    "        y_train = train_labels[train_idx]\n",
    "        x_valid = train_images[valid_idx]\n",
    "        y_valid = train_labels[valid_idx]\n",
    "        \n",
    "        num_folds += 1\n",
    "        print(\"start Kfold number {0} from {1}\".format(num_folds, nfolds))\n",
    "        print(\"slpit train: \", len(x_train), len(y_train))\n",
    "        print(\"split valid: \", len(x_valid), len(y_valid))\n",
    "        \n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_acc', patience=3, verbose=0)\n",
    "        ]\n",
    "        model.fit(x_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, shuffle=True,\n",
    "                  verbose=2, validation_data=(x_valid, y_valid), callbacks=callbacks)\n",
    "                  \n",
    "        prediction_valid = model.predict(x_valid.astype('float32'), batch_size= batch_size, verbose=2)\n",
    "        \n",
    "        score = accuracy_score(np.argmax(y_valid, axis =1), \n",
    "                               np.argmax(prediction_valid, axis = 1))\n",
    "        print(\"accuracy_score = \", score)\n",
    "        sum_score += score*len(valid_idx)\n",
    "        \n",
    "        #store valid preditcion\n",
    "        for i in range(len(valid_idx)):\n",
    "            yfull_train[valid_idx[i]] = prediction_valid[i]\n",
    "            \n",
    "        models.append(model) \n",
    "     \n",
    "    score = sum_score / len(train_images)\n",
    "    print(\"accuracy_score_train indepent avg\", score)\n",
    "    \n",
    "    info_string = 'acc' +str(score) + '_folds_' + str(nfolds) + '_ep_' + str(nb_epoch)\n",
    "    return info_string, models, Y_train\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_several_folds_mean(data, nfolds):\n",
    "    a = np.array(data[0])\n",
    "    for i in range(1, nfolds):\n",
    "        a += np.array(data[i])\n",
    "    a /= nfolds\n",
    "    return a.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(predictions, test_id, info, Y_train):\n",
    "    predictions = np.argmax(predictions, axis= 1)    \n",
    "    y_maps = dict()\n",
    "    y_maps = {v:k for k, v in Y_train.items()}\n",
    "    pred_labels = [y_maps[k] for k in predictions]\n",
    "                            \n",
    "    sub1 = pd.DataFrame({'test_id':test_id, 'label':pred_labels})    \n",
    "    sub_file = 'submission_' + info + '.csv'\n",
    "    sub1.to_csv(sub_file, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run Kfold validatin process on test data using trained model \n",
    "take mean of kfoled predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfold_validatin_on_test(info_string, models, Y_train):\n",
    "    num_folds = 0\n",
    "    batch_size = 16\n",
    "    yfull_test = []\n",
    "    nfolds = len(models)\n",
    "    \n",
    "    test_data, test_id = read_and_normalize_test_data()\n",
    "    \n",
    "    for i in range(nfolds):\n",
    "        num_folds += 1\n",
    "        model = models[i]\n",
    "        print(\"starts Kfold number {0} from {1}\".format(num_folds, nfolds))        \n",
    "        test_predition = model.predict(test_data, batch_size=batch_size, verbose=2)\n",
    "        yfull_test.append(test_predition)\n",
    "        \n",
    "    test_res = merge_several_folds_mean(data=yfull_test, nfolds=nfolds)\n",
    "   \n",
    "    create_submission(predictions=test_res, test_id=test_id, info=info_string, Y_train =Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if  __name__ == '__main__':\n",
    "    st = time.time()\n",
    "    print(\"Keras verstion {}\".format(keras_version))\n",
    "    nfolds = 5\n",
    "    info_string, models, Y_train = run_kfold_on_created_model(nfolds)\n",
    "    run_kfold_validatin_on_test(info_string, models, Y_train)\n",
    "    print(\"completly run model: {} seconds\".format(round(time.time() - st, 2))) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('submission_1.csv')\n",
    "sub.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
