{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.0.8'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2017)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import glob\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from skimage import transform, util\n",
    "from skimage import filters, color\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import __version__ as keras_version\n",
    "keras_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#size of image\n",
    "img_size = 128\n",
    "#num of collor changls \n",
    "num_channels = 3\n",
    "#conv filter row and col\n",
    "f_row = f_col = 3\n",
    "#number of filters for convolution layer1\n",
    "num_filter1 = 16\n",
    "#number of filters for convolution layer2\n",
    "num_filter2 = 36\n",
    "\n",
    "#folders = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and resizeing images using OpenCv - cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_img_cv2(path):\n",
    "    img = cv2.imread(path)\n",
    "    #print(img.shape)\n",
    "    #img = util.img_as_float(img)\n",
    "    #eimg =  filters.sobel(color.rgb2gray(img)) #signify importance of fixes\n",
    "    #gray = color.rgb2gray(img)\n",
    "    #out = transform.seam_carve(gray, eimg, 'vertical',  500 )\n",
    "   \n",
    "    resized = cv2.resize(img, (img_size, img_size), cv2.INTER_LINEAR)\n",
    "    return resized\n",
    "    #return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = os.path.join('input','train', 'ALB', '*.jpg')\n",
    "files = glob.glob(path)\n",
    "fl = files[1]\n",
    "img = get_img_cv2(fl)\n",
    "print(img.shape)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Loading training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "    train_images = []\n",
    "    train_ids = []\n",
    "    train_labels = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Loading train images\")\n",
    "    folders = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "    for idx, fld in enumerate(folders):\n",
    "        print(\"Load folder: {0},\\t\\t Index: {1}\".format(fld, idx))\n",
    "        path = os.path.join('input','train', fld, '*.jpg')\n",
    "        files = glob.glob(path)\n",
    "        for fl in files:\n",
    "            flbase = os.path.basename(fl)\n",
    "            img = get_img_cv2(fl)\n",
    "            train_images.append(img)\n",
    "            train_ids.append(flbase)\n",
    "            train_labels.append(idx)\n",
    "            \n",
    "    print(\"Read train data time: {} seconds\".format(round(time.time() - start_time, 2))) \n",
    "    return train_images, train_labels, train_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_test_data():\n",
    "    start_time = time.time()\n",
    "    test_images = []\n",
    "    test_ids = []\n",
    "    \n",
    "    path = os.path.join('input','test_stg1','*.jpg')\n",
    "    files = sorted(glob.glob(path))\n",
    "    for fl in files:\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_img_cv2(fl)\n",
    "        test_images.append(img)\n",
    "        test_ids.append(flbase)\n",
    "    print(\"Read test data time: {} seconds\".format(round(time.time() - start_time, 2)))    \n",
    "    return test_images, test_ids     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read and normalize train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_normalize_train_data():\n",
    "    train_images, train_labels, train_ids = load_train_data()    \n",
    "    \n",
    "    print(\"Conver to numpy array\")\n",
    "    train_images = np.array(train_images, dtype = np.uint8)\n",
    "    train_labels = np.array(train_labels, dtype = np.uint8)\n",
    "    \n",
    "    print(\"Reshape train_images\")\n",
    "    train_images = train_images.transpose((0, 3, 1, 2))\n",
    "    \n",
    "    print(\"Normalize and Convert to float\")\n",
    "    train_images = train_images.astype('float32')\n",
    "    train_images = train_images / 255\n",
    "    train_labels = np_utils.to_categorical(train_labels, 8)\n",
    "    \n",
    "    print(\"Train shape :\", train_images.shape)\n",
    "    print(train_images.shape[0], \"train samples\")\n",
    "    return train_images, train_labels, train_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read and normalize test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_and_normalize_test_data():\n",
    "    test_images, test_id = load_test_data()\n",
    "    \n",
    "    print(\"Convert to numpy array\")\n",
    "    test_images = np.array(test_images, dtype = np.uint8)\n",
    "    \n",
    "    print(\"Reshape test images\")\n",
    "    #test_images = test_images.transpose((0,3,1,2))\n",
    "    test_images = test_images.transpose((0,1,2)) #gray image\n",
    "    \n",
    "    print(\"Convert to float and normalize\")\n",
    "    test_images = test_images.astype('float32')\n",
    "    test_images = test_images / 255\n",
    "    \n",
    "    print(\"Test Shape: \", test_images.shape)\n",
    "    print(test_images.shape[0], \"test samples\")\n",
    "    return test_images, test_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D(padding=(1,1), input_shape = (num_channels, img_size, img_size), dim_ordering='th'))\n",
    "    model.add(Convolution2D(nb_filter=num_filter1, nb_row=f_row, nb_col=f_col, activation='relu', dim_ordering='th'))\n",
    "    model.add(ZeroPadding2D(padding=(1,1), dim_ordering='th'))\n",
    "    model.add(Convolution2D(nb_filter=num_filter1, nb_row=f_row, nb_col=f_col, activation='relu', dim_ordering='th'))          \n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), dim_ordering='th'))  \n",
    "    model.add(Dropout(0.25))          \n",
    "              \n",
    "    model.add(ZeroPadding2D(padding=(1,1), dim_ordering='th')) \n",
    "    model.add(Convolution2D(nb_filter=num_filter2, nb_row=f_row, nb_col=f_col, activation='relu', dim_ordering='th'))  \n",
    "    model.add(ZeroPadding2D(padding=(1,1), dim_ordering='th'))  \n",
    "    model.add(Convolution2D(nb_filter=num_filter2, nb_row=f_row, nb_col=f_col, activation='relu', dim_ordering='th'))  \n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), dim_ordering='th'))  \n",
    "    model.add(Dropout(0.25))  \n",
    "              \n",
    "    model.add(Flatten())   \n",
    "    model.add(Dense(output_dim=128, activation='relu'))  \n",
    "    model.add(Dropout(0.5))  \n",
    "    model.add(Dense(output_dim=128, activation='relu')) \n",
    "    model.add(Dropout(0.5))  \n",
    "    model.add(Dense(output_dim=8, activation='softmax')) \n",
    "              \n",
    "    sgd = SGD(lr=0.1, decay= 1e-6, momentum=0.9, nesterov=True)   \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = sgd)  \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Running Kfold cross validation on created model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfold_on_created_model(nfolds = 10):\n",
    "    #model fit input dimentions\n",
    "    batch_size = 32\n",
    "    nb_epoch = 10\n",
    "    random_state = 51\n",
    "    \n",
    "    train_images, train_labels, train_ids = read_and_normalize_train_data()\n",
    "    yfull_train = dict()\n",
    "    kfold = KFold(len(train_images), n_folds=nfolds, shuffle=True, random_state=random_state)\n",
    "    num_folds = 0\n",
    "    sum_score = 0\n",
    "    models =[]\n",
    "    for train_idx, valid_idx in kfold:\n",
    "        model = create_model()\n",
    "        x_train = train_images[train_idx]\n",
    "        y_train = train_labels[train_idx]\n",
    "        x_valid = train_images[valid_idx]\n",
    "        y_valid = train_labels[valid_idx]\n",
    "        \n",
    "        num_folds += 1\n",
    "        print(\"start Kfold number {0} from {1}\".format(num_folds, nfolds))\n",
    "        print(\"slpit train: \", len(x_train), len(y_train))\n",
    "        print(\"split valid: \", len(x_valid), len(y_valid))\n",
    "        \n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=3, verbose=0)\n",
    "        ]\n",
    "        model.fit(x_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, shuffle=True,\n",
    "                  verbose=2, validation_data=(x_valid, y_valid), callbacks=callbacks)\n",
    "                  \n",
    "        prediction_valid = model.predict(x_valid.astype('float32'), batch_size= batch_size, verbose=2)\n",
    "        score = log_loss(y_valid, prediction_valid)\n",
    "        print(\"score log_loss = \", score)\n",
    "        sum_score += score*len(valid_idx)\n",
    "        \n",
    "        #store valid preditcion\n",
    "        for i in range(len(valid_idx)):\n",
    "            yfull_train[valid_idx[i]] = prediction_valid[i]\n",
    "            \n",
    "        models.append(model) \n",
    "     \n",
    "    score = sum_score / len(train_images)\n",
    "    print(\"log_loss_train indepent avg\", score)\n",
    "    \n",
    "    info_string = 'loss_' +str(score) + '_folds_' + str(nfolds) + '_ep_' + str(nb_epoch)\n",
    "    return info_string, models\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_several_folds_mean(data, nfolds):\n",
    "    a = np.array(data[0])\n",
    "    for i in range(1, nfolds):\n",
    "        a += np.array(data[i])\n",
    "    a /= nfolds\n",
    "    return a.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_submission(predictions, test_id, info):\n",
    "    result = pd.DataFrame(predictions, columns =['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT'])\n",
    "    result.loc[:, 'image'] = pd.Series(test_id, index = result.index)\n",
    "    sub_file = 'submission_' + info + '.csv'\n",
    "    result.to_csv(sub_file, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run Kfold validatin process on test data using trained model \n",
    "take mean of kfoled predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_kfold_validatin_on_test(info_string, models):\n",
    "    num_folds = 0\n",
    "    batch_size = 16\n",
    "    yfull_test = []\n",
    "    nfolds = len(models)\n",
    "    \n",
    "    for i in range(nfolds):\n",
    "        num_folds += 1\n",
    "        model = models[i]\n",
    "        print(\"starts Kfold number {0} from {1}\".format(num_folds, nfolds))\n",
    "        test_data, test_id = read_and_normalize_test_data()\n",
    "        test_predition = model.predict(test_data, batch_size=batch_size, verbose=2)\n",
    "        yfull_test.append(test_predition)\n",
    "        \n",
    "    test_res = merge_several_folds_mean(data=yfull_test, nfolds=nfolds)\n",
    "    info_string = 'loss_' + info_string + '_folds_' + str(nfolds)\n",
    "    \n",
    "    create_submission(predictions=test_res, test_id=test_id, info=info_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if  __name__ == '__main__':\n",
    "    st = time.time()\n",
    "    print(\"Keras verstion {}\".format(keras_version))\n",
    "    nfolds = 5\n",
    "    info_string, models = run_kfold_on_created_model(nfolds)\n",
    "    run_kfold_validatin_on_test(info_string, models)\n",
    "    print(\"completly run model: {} seconds\".format(round(time.time() - st, 2))) \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
