{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os, sys\n",
    "import glob\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from keras import __version__\n",
    "#from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Convolution2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IM_WIDTH, IM_HEIGHT = 64, 64 #fixed size for InceptionV3\n",
    "NB_EPOCHS = 10\n",
    "BAT_SIZE = 32\n",
    "FC_SIZE = 1024\n",
    "NB_IV3_LAYERS_TO_FREEZE = 172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('input/train.csv')\n",
    "test = pd.read_csv('input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to read image\n",
    "def read_img(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    img = cv2.resize(img, (IM_WIDTH, IM_HEIGHT))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train data has 25 unique labels\n"
     ]
    }
   ],
   "source": [
    "label_counts = train.label.value_counts()\n",
    "print ('The train data has {} unique labels'.format(train['label'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candy already save.\n",
      "chocolate already save.\n",
      "juice already save.\n",
      "coffee already save.\n",
      "tea already save.\n",
      "cereal already save.\n",
      "water already save.\n",
      "jam already save.\n",
      "spices already save.\n",
      "honey already save.\n",
      "chips already save.\n",
      "soda already save.\n",
      "pasta already save.\n",
      "tomatosauce already save.\n",
      "nuts already save.\n",
      "milk already save.\n",
      "cake already save.\n",
      "vinegar already save.\n",
      "rice already save.\n",
      "oil already save.\n",
      "beans already save.\n",
      "sugar already save.\n",
      "flour already save.\n",
      "fish already save.\n",
      "corn already save.\n"
     ]
    }
   ],
   "source": [
    "for lbl in label_counts.index:\n",
    "    #print(lbl)\n",
    "    save_to_dir = 'input/' + lbl\n",
    "    if not os.path.exists(save_to_dir):\n",
    "        os.mkdir(save_to_dir)\n",
    "    else:\n",
    "        print(lbl + ' already save.')\n",
    "        continue\n",
    "    img_id = train[train['label'] == lbl]['image_id'].values\n",
    "    n = 1000//len(img_id)\n",
    "    for img in tqdm(img_id):    \n",
    "        x = read_image(TRAIN_PATH + '{}.png'.format(img))\n",
    "        cv2.imwrite('input/' + lbl+ '/' + img +'.png', cv2.cvtColor(x, cv2.COLOR_RGB2BGR))\n",
    "        x = x.reshape((1,) + x.shape)\n",
    "        i = 0\n",
    "        for batch in datagen.flow(x, batch_size=1,\n",
    "                          save_to_dir= save_to_dir, save_prefix=lbl, save_format='png'):\n",
    "            i += 1\n",
    "            if i > n:\n",
    "                break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = []\n",
    "X_train_id = []\n",
    "y_train = []\n",
    "start_time = time.time()\n",
    "\n",
    "print('Read train images')\n",
    "folders = list(label_counts.index)\n",
    "for fld in folders:\n",
    "    index = folders.index(fld)\n",
    "    print('Load folder {} (Index: {})'.format(fld, index))\n",
    "    path = os.path.join('input', fld, '*.png')\n",
    "    #print(path)\n",
    "    files = glob.glob(path)\n",
    "    print(len(files))\n",
    "    for fl in files:\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = read_img(fl)\n",
    "        X_train.append(img)\n",
    "        X_train_id.append(flbase)\n",
    "        y_train.append(index)\n",
    "\n",
    "print('Read train data time: {} seconds'.format(round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(X_train, np.float32) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28770, 64, 64, 3) (28770,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, np.array(y_train).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_X, valid_X, train_y, valid_y = train_test_split(X_train, y_train, test_size = 0.25, random_state = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, valid_X, train_y, valid_y = train_test_split(X_train, y_train, test_size = 0.25, random_state = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_class = label_counts.shape[0]\n",
    "num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, (3,3), activation='relu', padding='same',input_shape = (64,64,3))) # if you resize the image above, shape would be (128,128,3)\n",
    "model.add(Convolution2D(32, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Convolution2D(64, (3,3), activation='relu', padding='same'))\n",
    "model.add(Convolution2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution2D(128, (3,3), activation='relu', padding='same'))\n",
    "model.add(Convolution2D(128, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(num_class, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20139 samples, validate on 8631 samples\n",
      "Epoch 1/30\n",
      "45s - loss: 2.7624 - acc: 0.1084 - val_loss: 9.9047 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "22s - loss: 2.4648 - acc: 0.2070 - val_loss: 12.1298 - val_acc: 5.7931e-04\n",
      "Epoch 3/30\n",
      "22s - loss: 2.1764 - acc: 0.2988 - val_loss: 14.6148 - val_acc: 0.0022\n",
      "Epoch 4/30\n",
      "22s - loss: 1.9369 - acc: 0.3768 - val_loss: 14.1348 - val_acc: 0.0022\n",
      "Epoch 5/30\n",
      "22s - loss: 1.7396 - acc: 0.4428 - val_loss: 14.6491 - val_acc: 0.0016\n",
      "Epoch 6/30\n",
      "22s - loss: 1.5472 - acc: 0.5013 - val_loss: 15.3544 - val_acc: 0.0031\n",
      "Epoch 7/30\n",
      "22s - loss: 1.3774 - acc: 0.5518 - val_loss: 15.3677 - val_acc: 0.0038\n",
      "Epoch 8/30\n",
      "22s - loss: 1.2074 - acc: 0.6062 - val_loss: 15.7469 - val_acc: 0.0050\n",
      "Epoch 9/30\n",
      "22s - loss: 1.0898 - acc: 0.6432 - val_loss: 15.7767 - val_acc: 0.0037\n",
      "Epoch 10/30\n",
      "22s - loss: 0.9735 - acc: 0.6775 - val_loss: 15.8470 - val_acc: 0.0048\n",
      "Epoch 11/30\n",
      "22s - loss: 0.8710 - acc: 0.7104 - val_loss: 15.8718 - val_acc: 0.0054\n",
      "Epoch 12/30\n",
      "22s - loss: 0.7766 - acc: 0.7443 - val_loss: 15.9046 - val_acc: 0.0058\n",
      "Epoch 13/30\n",
      "22s - loss: 0.7151 - acc: 0.7647 - val_loss: 15.9450 - val_acc: 0.0049\n",
      "Epoch 14/30\n",
      "22s - loss: 0.6530 - acc: 0.7833 - val_loss: 15.9397 - val_acc: 0.0048\n",
      "Epoch 15/30\n",
      "22s - loss: 0.6134 - acc: 0.7955 - val_loss: 15.9303 - val_acc: 0.0064\n",
      "Epoch 16/30\n",
      "22s - loss: 0.5639 - acc: 0.8117 - val_loss: 15.9558 - val_acc: 0.0052\n",
      "Epoch 17/30\n",
      "22s - loss: 0.5289 - acc: 0.8253 - val_loss: 15.9503 - val_acc: 0.0058\n",
      "Epoch 18/30\n",
      "22s - loss: 0.4886 - acc: 0.8356 - val_loss: 15.9695 - val_acc: 0.0058\n",
      "Epoch 19/30\n",
      "22s - loss: 0.4677 - acc: 0.8483 - val_loss: 15.9836 - val_acc: 0.0053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe78473d668>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stops = EarlyStopping(patience=3, monitor='val_acc')\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=BAT_SIZE, epochs=NB_EPOCHS, shuffle=True,\n",
    "                  verbose=2, validation_split=0.3, callbacks=[early_stops])\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
